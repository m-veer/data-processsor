# üß∞ Data Processor

**Event-driven, cloud-native log ingestion pipeline on Google Cloud**

Data Processor is a production-style reference project that shows how to build a **fully automated ingestion and processing system** on **Cloud Run, Pub/Sub, Firestore, and Terraform**, wired up with **GitHub Actions CI/CD**.

It consists of:

- **API Service (Cloud Run + FastAPI)** ‚Äì receives JSON or text logs, normalizes them, and publishes to Pub/Sub.
- **Worker Service (Cloud Run)** ‚Äì subscribes to Pub/Sub, simulates heavy processing, performs PII redaction, and stores results in Firestore with strict multi-tenant isolation.

---

## ‚ö° Quick Start (3 Steps)

> Best for just seeing everything run end-to-end in your own GCP project.

1. **Clone the repo & set env**
   ```bash
   git clone https://github.com/m-veer/data-processor.git
   cd data-processor

   # Create .env at repo root
   cat > .env << EOF
   GCP_PROJECT_ID=your-project-id
   GCP_REGION=us-central1
   EOF

2. Provision infra with Terraform
cd terraform
terraform init
terraform plan -out=tfplan
terraform apply tfplan

3. Trigger GitHub Actions deployment
- Push to a feature branch ‚Üí open PR ‚Üí PR validation runs.
- Merge to main ‚Üí merge workflow builds Docker images, pushes to Artifact Registry, and deploys Cloud Run + Pub/Sub + Firestore via Terraform.
- Grab the API Cloud Run URL from the merge workflow summary and start ingesting logs.

üìñ Introduction
This project is designed as a realistic backend / DevOps portfolio piece:
- Cloud-native, serverless-first architecture
- Infrastructure as Code with Terraform
- CI/CD via GitHub Actions
- Multi-tenant Firestore data model
- Built-in crash / retry / DLQ simulation for reliability demos

You get:
1. Unified ingestion endpoint for JSON & text logs
2. Asynchronous worker with PII redaction & heavy-processing simulation
3. Dead-Letter Queue (DLQ) routing using Pub/Sub delivery attempts

üèó Architecture Overview
High-Level Flow

           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ      Client / Postman      ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚îÇ  HTTPS /ingest
                        ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ  API Service (FastAPI)‚îÇ  Cloud Run
              ‚îÇ  - JSON / text ingest ‚îÇ
              ‚îÇ  - Multi-tenant IDs   ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ Pub/Sub publish
                         ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ   Pub/Sub Topic              ‚îÇ
           ‚îÇ   data-ingestion             ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ subscription
                       ‚ñº
          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
          ‚îÇ Worker Service (Python)       ‚îÇ Cloud Run
          ‚îÇ - Simulated heavy processing  ‚îÇ
          ‚îÇ - PII redaction               ‚îÇ
          ‚îÇ - Crash + retry simulation    ‚îÇ
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ Firestore write
                      ‚ñº
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Firestore                            ‚îÇ
         ‚îÇ tenants/{tenant_id}/processed_logs/  ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

                 ‚ñ≤
                 ‚îÇ  after N failed deliveries
                 ‚îÇ  (e.g. 20 attempts)
                 ‚îÇ
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ Pub/Sub DLQ Topic            ‚îÇ
           ‚îÇ data-ingestion-dlq           ‚îÇ
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚ú® Core Features

1Ô∏è‚É£ Unified Ingestion API
- OST /ingest
- Accepts:
    - application/json with tenant_id, optional log_id, text
    - text/plain with X-Tenant-ID header
- Normalizes all input into a single internal JSON structure and publishes to Pub/Sub.

2Ô∏è‚É£ Asynchronous Worker with Crash Simulation
- Subscribes to data-ingestion Pub/Sub topic.
- Simulates heavy processing (sleep based on text length).
- Redacts phone numbers from text (XXX-XXX-XXXX, XXX-XXXX, etc.).
- For messages containing crash_test, intentionally fails first 5 attempts, then succeeds using Pub/Sub‚Äôs delivery_attempt counter.
- Persists processed logs to Firestore:
    - tenants/{tenant_id}/processed_logs/{log_id}

3Ô∏è‚É£ Reliability with Dead-Letter Queue (DLQ)
- Terraform configures:
    - DLQ topic: data-ingestion-dlq
    - Subscription settings: max_delivery_attempts = 20
- Messages that keep failing (e.g., bugs, malformed data) are automatically moved to DLQ for inspection / replay.

üß∞ Tech Stack
- Runtime & Services
    - Python (FastAPI + Pub/Sub client + Firestore client)
    - Google Cloud Run (API & Worker)
    - Google Pub/Sub (topic, subscription, DLQ)
    - Google Firestore (multi-tenant document storage)
    - Artifact Registry (Docker images)

- Infrastructure
    - Terraform (GCP provider)
    - Google Cloud IAM, service accounts
    - Terraform state backend in GCS

- CI/CD
    - GitHub Actions:
        - PR validation (pr-workflow.yml)
        - Image build & deploy (deploy-api.yml, deploy-worker.yml, merge-workflow.yml)
        - Infra deploy (terraform-deploy.yml)

- Local Tooling
  - docker-compose.local.yml
  - Makefile helpers
  - Shell scripts (format_check.sh, log_tail.sh, test_crash_recovery.sh, etc.)

üìÅ Project Structure
.
‚îú‚îÄ‚îÄ .github/
‚îÇ   ‚îî‚îÄ‚îÄ workflows/
‚îÇ       ‚îú‚îÄ‚îÄ deploy-api.yml          # Deploy only API service
‚îÇ       ‚îú‚îÄ‚îÄ deploy-worker.yml       # Deploy only Worker service
‚îÇ       ‚îú‚îÄ‚îÄ deploy.yml              # Generic deploy workflow
‚îÇ       ‚îú‚îÄ‚îÄ merge-workflow.yml      # Main branch: build & deploy infra + services
‚îÇ       ‚îú‚îÄ‚îÄ pr-workflow.yml         # PR validation (tests, format, TF validate)
‚îÇ       ‚îî‚îÄ‚îÄ terraform-deploy.yml    # Terraform apply workflow
‚îÇ
‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # API container image
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # FastAPI app (Pub/Sub publisher + /ingest)
‚îÇ   ‚îú‚îÄ‚îÄ load_test_local.py          # Local load testing helper
‚îÇ   ‚îú‚îÄ‚îÄ run_local.py                # Run API locally with uvicorn
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # API Python deps
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Pytest config
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îî‚îÄ‚îÄ test_main.py            # API unit tests
‚îÇ
‚îú‚îÄ‚îÄ worker/
‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile                  # Worker container image
‚îÇ   ‚îú‚îÄ‚îÄ main.py                     # Pub/Sub subscriber + Firestore writer
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt            # Worker Python deps
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py                 # Pytest config
‚îÇ   ‚îî‚îÄ‚îÄ tests/
‚îÇ       ‚îî‚îÄ‚îÄ test_main.py            # Worker unit tests
‚îÇ
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf                     # Core infra: Run, Pub/Sub, Firestore, IAM
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                # Terraform variables
‚îÇ   ‚îú‚îÄ‚îÄ outputs.tf                  # Output URLs, IDs, etc.
‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfvars            # Project-specific values
‚îÇ   ‚îú‚îÄ‚îÄ terraform.tfstate*          # Local state (if not remote)
‚îÇ   ‚îú‚îÄ‚îÄ .terraform/                 # Terraform cache
‚îÇ   ‚îú‚îÄ‚îÄ .terraform.lock.hcl         # Provider lock file
‚îÇ   ‚îú‚îÄ‚îÄ environments/               # (Optional) extra env configs
‚îÇ   ‚îú‚îÄ‚îÄ import_resources.sh         # Helper for importing existing GCP resources
‚îÇ   ‚îî‚îÄ‚îÄ tfplan                      # Saved TF plan (when used)
‚îÇ
‚îú‚îÄ‚îÄ docker-compose.local.yml        # Local multi-service run config
‚îú‚îÄ‚îÄ format_check.sh                 # Local formatting helper
‚îú‚îÄ‚îÄ deploy.sh                       # Local deploy helper
‚îú‚îÄ‚îÄ rebuild_and_deploy.sh           # Local rebuild + deploy script
‚îú‚îÄ‚îÄ log_tail.sh                     # Tail Cloud Run logs helper
‚îú‚îÄ‚îÄ test_crash_recovery.sh          # Script to drive crash_test scenarios
‚îÇ
‚îú‚îÄ‚îÄ architecture.md                 # Extended architecture notes
‚îú‚îÄ‚îÄ SETUP.md                        # Detailed setup instructions
‚îú‚îÄ‚îÄ TERRAFORM_SETUP.md              # Deep dive Terraform instructions
‚îú‚îÄ‚îÄ deploy.md                       # Deployment notes / runbook
‚îÇ
‚îú‚îÄ‚îÄ Back End Interview.pdf          # Problem statement / interview brief
‚îú‚îÄ‚îÄ data-processor-480019-sa-key.json # (local) GCP SA key for testing
‚îú‚îÄ‚îÄ .env                            # Local environment variables
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ Makefile
‚îú‚îÄ‚îÄ README.md                       # You are here
‚îî‚îÄ‚îÄ Todo.txt                        # Future polish & task list

üë®‚Äçüíª New Developer Setup
This is the ‚ÄúI just joined the team, what do I do?‚Äù section.

1. Install prerequisites
- Python 3.11+
- Docker & Docker Compose
- Terraform ‚â• 1.6
- gcloud CLI (with your GCP account authenticated)

2. Configure GCP project
- gcloud config set project <YOUR_PROJECT_ID>
- gcloud auth application-default login

3. Fill in Terraform variables
Edit terraform/terraform.tfvars (example):
- project_id        = "your-project-id"
- region            = "us-central1"
- api_image         = "us-central1-docker.pkg.dev/your-project-id/data-processor/data-processor-api:latest"
- worker_image      = "us-central1-docker.pkg.dev/your-project-id/data-processor/data-processor-worker:latest"
- firestore_db_name = "(default)"

4. Initialize & deploy infra
- cd terraform
- terraform init
- erraform plan -out=tfplan
- terraform apply tfplan

5. Let GitHub Actions manage deployments
- Configure repository secrets (service account JSON, project ID, region, etc.).
- Push feature branches ‚Üí PR validation.
- Merge to main ‚Üí automatic build & deploy.

6. Run tests locally
- # From repo root
- pytest api/tests
- pytest worker/tests

üß™ Local Development & Testing
- Run API locally
    - cd api
    - python -m venv venv
    - source venv/bin/activate
    - pip install -r requirements.txt
    - uvicorn main:app --reload --host 0.0.0.0 --port 8080

- Run Worker locally (against real Pub/Sub)
    - Make sure you have valid GCP credentials & env vars:
        - cd worker
        - python -m venv venv
        - source venv/bin/activate
        - pip install -r requirements.txt

        - export GCP_PROJECT_ID=<your-project-id>
        - export PUBSUB_SUBSCRIPTION_ID=data-ingestion-sub

        - python main.py

- Run everything with Docker Compose
    - docker-compose -f docker-compose.local.yml up --build

üåê API Usage
1. JSON Ingestion
    - Request
        - POST /ingest HTTP/1.1
        - Content-Type: application/json

        - {
            "tenant_id": "abcd",
            "log_id": "test_012",
            "text": "Testing crash_test path with phone 555-0199"
        - }

    - Response
        - {
            "status": "accepted",
            "tenant_id": "abcd",
            "log_id": "test_012",
            "message_id": "17229986288873513",
            "message": "Data queued for processing"
        - }

2. Text Ingestion
    - Request
        - POST /ingest HTTP/1.1
        - Content-Type: text/plain
        - X-Tenant-ID: abcd

        - User 555-0199 accessed the system from IP 192.168.1.1 - TXT Request #1 - crash_test

    - Response
        - {
            "status": "accepted",
            "tenant_id": "abcd",
            "log_id": "bf4962a1-c55b-4dc4-b6a6-9476a27e16a2",
            "message_id": "17135587711641859",
            "message": "Data queued for processing"
        - }

üßπ PII Redaction
- worker/main.py uses redact_pii(text: str) -> str to scrub phone numbers.

Handled patterns include:
- XXX-XXX-XXXX ‚Üí [REDACTED]
- XXX-XXXX ‚Üí [REDACTED]

You can extend this function to cover:
- International formats (+1 (555) 123-4567)
- Email addresses
- Credit card patterns
- Custom tenant-specific rules

üîÅ Crash Simulation & Recovery
- Messages whose text contains crash_test are treated specially:
delivery_attempt = message.delivery_attempt or 1

- if "crash_test" in text.lower():
      if delivery_attempt <= 5:
          # Simulate crash
          raise Exception(f"Simulated crash - Attempt {delivery_attempt}")
      else:
          # Finally succeed
          logger.info(f"‚úÖ PASSED after {delivery_attempt} attempts")

- On each failure:
    - Worker logs the error
    - Calls message.nack() so Pub/Sub retries later
- On attempt 6+, processing proceeds normally:
    - Heavy processing simulation
    - PII redaction
    - Firestore write
    - message.ack()

- Testing crash scenarios
    - Use test_crash_recovery.sh (or Postman) to send payloads with crash_test and watch:
        - ./log_tail.sh worker   # Tail Cloud Run worker logs
        - ./test_crash_recovery.sh

‚ò†Ô∏è Dead-Letter Queue (DLQ) Behavior
Terraform configures:
- google_pubsub_topic.data_ingestion_dlq (data-ingestion-dlq)
- google_pubsub_subscription.data_ingestion_sub with:
    - dead_letter_policy referencing DLQ topic
    - max_delivery_attempts = 20
- After 20 failed deliveries:
    - The message stops retrying on the main subscription.
    - It is sent to data-ingestion-dlq for inspection.

- You can pull DLQ messages with:
    - gcloud pubsub subscriptions pull data-ingestion-dlq-sub \
        --project=$PROJECT_ID \
        --auto-ack \
        --limit=10

üß± Terraform Notes
- Initialize
    - cd terraform
    - terraform init
- Plan & Apply
    - terraform plan -out=tfplan \
      -var="project_id=<PROJECT_ID>" \
      -var="region=us-central1"
    - terraform apply tfplan

- Import Existing Resources
- If some resources were created manually (e.g. DLQ topic), import them:
    - terraform import \
        google_pubsub_topic.data_ingestion_dlq \
        "projects/<PROJECT_ID>/topics/data-ingestion-dlq"
- import_resources.sh contains helper commands you can adapt.

- For deeper details, see TERRAFORM_SETUP.md.

üöÄ Future Scope
- Some planned / potential enhancements for this project:
- Platform & Architecture
    - Multi-region deployments with global load balancing for API and Worker.
    - BigQuery sink for analytical queries on processed logs.
    - Event versioning & schema registry (e.g., using JSON schema or Proto) to evolve message formats safely.
    - Config-driven PII redaction rules stored per tenant in Firestore or a config service.
    - Idempotent processing (e.g., dedupe by (tenant_id, log_id) with strong guarantees).
- Observability & Operations
    - End-to-end tracing using OpenTelemetry (API ‚Üí Pub/Sub ‚Üí Worker ‚Üí Firestore).
    - Dashboards & alerts in Cloud Monitoring for:
        - Pub/Sub backlog
        - DLQ volume
        - Error rates per tenant
    - SLOs & error budgets (e.g., 99.9% successful processing within X minutes).
    - Admin tooling to replay DLQ messages back into the main topic.
- Security & Multi-Tenancy
    - Tenant-aware authentication & authorization (e.g., OAuth / API keys / mTLS).
    - Per-tenant quotas & rate limiting to isolate noisy neighbors.
    - Customer-managed encryption keys (CMEK) for regulatory use-cases.
    - Fine-grained Firestore security rules or access layer enforcing tenant isolation.
- Developer Experience
    - Local Pub/Sub & Firestore emulators wired into docker-compose.local.yml.
    - Smoke-test workflow that runs after every deploy using the real Cloud Run URL.
    - Scaffold scripts to create new environments (dev / staging / prod) from templates.
    - More test coverage for edge cases (PII patterns, DLQ routing, retry behavior).

ü§ù Contributing
1. Fork the repository.
2. Create a feature branch:
    - git checkout -b feature/my-change
3. Run tests and formatters locally:
    - pytest api/tests worker/tests
    - ./format_check.sh
4. Push and open a Pull Request.

- PRs automatically run the PR validation workflow (tests + Terraform checks).

üë§ Author
- Mayur Veer
    - GitHub: @m-veer
    - LinkedIn: linkedin.com/in/mayur-veer

<div align="center">
Built as a production-style backend & DevOps showcase.
Logs in, insights out.
</div> ``` ::contentReference[oaicite:0]{index=0}